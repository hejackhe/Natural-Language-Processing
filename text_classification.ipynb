{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classification for social media - He Tianyou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras, argmax\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "#from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier part one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Read training data\n",
    "train_data = pd.read_csv('semeval-tweets/twitter-training-data.txt', sep='\\\\t', names=['tweet_id','sentiment','tweet_text'])\n",
    "# Read dev data\n",
    "dev_data = pd.read_csv('semeval-tweets/twitter-dev-data.txt', sep='\\\\t', names=['tweet_id','sentiment','tweet_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>335104872099066692</td>\n",
       "      <td>positive</td>\n",
       "      <td>Felt privileged to play Foo Fighters songs on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>796528524030124618</td>\n",
       "      <td>positive</td>\n",
       "      <td>\" Pakistan may be an Islamic country, but der ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>760964834217238632</td>\n",
       "      <td>positive</td>\n",
       "      <td>Happy Birthday to the coolest golfer in Bali! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>147713180324524046</td>\n",
       "      <td>negative</td>\n",
       "      <td>TMILLS is going to Tucson! But the 29th and i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>732302280474120023</td>\n",
       "      <td>negative</td>\n",
       "      <td>Hmmmmm where are the #BlackLivesMatter when ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id sentiment  \\\n",
       "0  335104872099066692  positive   \n",
       "1  796528524030124618  positive   \n",
       "2  760964834217238632  positive   \n",
       "3  147713180324524046  negative   \n",
       "4  732302280474120023  negative   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  Felt privileged to play Foo Fighters songs on ...  \n",
       "1  \" Pakistan may be an Islamic country, but der ...  \n",
       "2  Happy Birthday to the coolest golfer in Bali! ...  \n",
       "3   TMILLS is going to Tucson! But the 29th and i...  \n",
       "4  Hmmmmm where are the #BlackLivesMatter when ma...  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove URLs. Note that URLs may appear in different forms\n",
    "train_data['tweet_text'] = train_data['tweet_text'].str.replace(r'(((https?:\\/\\/)|(w{3}.))[\\S]*)|([\\w\\d\\/\\.]*\\.(com|cn|co|net|org|edu|uk|int|js|html))', '')\n",
    "dev_data['tweet_text'] = dev_data['tweet_text'].str.replace(r'(((https?:\\/\\/)|(w{3}.))[\\S]*)|([\\w\\d\\/\\.]*\\.(com|cn|co|net|org|edu|uk|int|js|html))', '')\n",
    "# change all n't or n' suffixes to not e.g \"won't\",'wouldn',\"wouldn't\" into wo not\n",
    "train_data['tweet_text'] = train_data['tweet_text'].str.replace(r'(n\\'t|n\\')', ' not ')\n",
    "dev_data['tweet_text'] = dev_data['tweet_text'].str.replace(r'(n\\'t|n\\')', ' not ')\n",
    "# change all happy emojis to happy e.g :) :') ;) :D ;D :'D xD (: (':')):3 c: C: c; C; c': C':\n",
    "train_data['tweet_text'] = train_data['tweet_text'].str.replace(r\"\\:\\)|\\:\\'\\)|\\;\\)|\\:D|xD|\\:3|\\(\\:|\\('\\:|\\;D|\\:\\'D|c\\:|C\\:|c\\;|C\\;|c\\'\\:|C\\'\\:\", ' happy ')\n",
    "dev_data['tweet_text'] = dev_data['tweet_text'].str.replace(r\"\\:\\)|\\:\\'\\)|\\;\\)|\\:D|xD|\\:3|\\(\\:|\\('\\:|\\;D|\\:\\'D|c\\:|C\\:|c\\;|C\\;|c\\'\\:|C\\'\\:\", ' happy ')\n",
    "# change all sad emojis to sad e.g D; D: ): ); :'( D': Dx :( :'(' :c :C ;c ;C:'c:'C\n",
    "train_data['tweet_text'] = train_data['tweet_text'].str.replace(r\"D\\;|D\\:|\\)\\:|\\)\\;|\\:\\'\\(|D\\'\\:|Dx|\\:\\(|\\:c|\\:C|\\;c|\\;C|\\:\\'c|\\:\\'C\", ' sad ')\n",
    "dev_data['tweet_text'] = dev_data['tweet_text'].str.replace(r\"D\\;|D\\:|\\)\\:|\\)\\;|\\:\\'\\(|D\\'\\:|Dx|\\:\\(|\\:c|\\:C|\\;c|\\;C|\\:\\'c|\\:\\'C\", ' sad ')\n",
    "# remove twitter handles\n",
    "train_data['tweet_text'] = train_data['tweet_text'].str.replace(r'\\@[\\S]*', '')\n",
    "dev_data['tweet_text'] = dev_data['tweet_text'].str.replace(r'\\@[\\S]*', '')\n",
    "# Remove numbers that are fully made of digits\n",
    "train_data['tweet_text'] = train_data['tweet_text'].str.replace(r'\\b\\d+\\b','')\n",
    "dev_data['tweet_text'] = dev_data['tweet_text'].str.replace(r'\\b\\d+\\b','')\n",
    "# Remove words with only 1 character. \n",
    "train_data['tweet_text'] = train_data['tweet_text'].str.replace(r'\\b\\w\\b','')\n",
    "dev_data['tweet_text'] = dev_data['tweet_text'].str.replace(r'\\b\\w\\b','')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF Vectorizer\n",
    "stopset = set(stopwords.words('english')) # remove stopwords\n",
    "stopset.remove('not') # we keep not in our data to maintain negation\n",
    "stopset.add('wo') # to account for won't -> wo not after regex\n",
    "vectorizer = TfidfVectorizer(use_idf=True, lowercase=True, strip_accents='ascii', stop_words=stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'wo',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'positive', 'positive', ..., 'positive', 'positive',\n",
       "       'neutral'], dtype=object)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment becomes dependent variable\n",
    "train_y = np.array(train_data['sentiment'])\n",
    "dev_y = np.array(dev_data['sentiment'])\n",
    "\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = vectorizer.fit_transform(train_data.tweet_text)\n",
    "dev_X = vectorizer.transform(dev_data.tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45101,)\n",
      "(45101, 42076)\n",
      "(2000,)\n",
      "(2000, 42076)\n"
     ]
    }
   ],
   "source": [
    "print(train_y.shape)\n",
    "print(train_X.shape)\n",
    "print(dev_y.shape)\n",
    "print(dev_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.655\n",
      "[[439 252  12]\n",
      " [152 714  53]\n",
      " [ 36 185 157]]\n"
     ]
    }
   ],
   "source": [
    "# train MaxEnt classifier (Logisitic regression)\n",
    "clf = linear_model.LogisticRegression(multi_class = 'ovr')\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# test model accuracy\n",
    "pred_y_mxe = clf.predict(dev_X)\n",
    "acc_score_mxe = accuracy_score(dev_y, pred_y_mxe)\n",
    "conf_mat_mxe = confusion_matrix(dev_y, pred_y_mxe, labels = [\"positive\", \"neutral\", \"negative\"])\n",
    "\n",
    "print(acc_score_mxe)\n",
    "print(conf_mat_mxe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5995\n",
      "[[395 307   1]\n",
      " [146 763  10]\n",
      " [ 40 297  41]]\n"
     ]
    }
   ],
   "source": [
    "# train naive bayes classifier\n",
    "clf2 = naive_bayes.MultinomialNB()\n",
    "clf2.fit(train_X, train_y)\n",
    "\n",
    "# test model accuracy\n",
    "pred_y_nb = clf2.predict(dev_X)\n",
    "acc_score_nb = accuracy_score(dev_y, pred_y_nb)\n",
    "conf_mat_nb = confusion_matrix(dev_y, pred_y_nb, labels = [\"positive\", \"neutral\", \"negative\"])\n",
    "\n",
    "print(acc_score_nb)\n",
    "print(conf_mat_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5875\n",
      "[[359 342   2]\n",
      " [119 794   6]\n",
      " [ 44 312  22]]\n"
     ]
    }
   ],
   "source": [
    "# train svm\n",
    "#clf3 = svm.SVC(gamma='auto')\n",
    "clf3 = linear_model.SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "clf3.fit(train_X, train_y)\n",
    "\n",
    "# test model accuracy\n",
    "pred_y_svm = clf3.predict(dev_X)\n",
    "acc_score_svm = accuracy_score(dev_y, pred_y_svm)\n",
    "conf_mat_svm = confusion_matrix(dev_y, pred_y_svm, labels = [\"positive\", \"neutral\", \"negative\"])\n",
    "\n",
    "print(acc_score_svm)\n",
    "print(conf_mat_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM classifier with GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment becomes dependent variable\n",
    "#train_y = np.array(train_data['sentiment'])\n",
    "#dev_y = np.array(dev_data['sentiment'])\n",
    "#label_encoder = LabelEncoder()\n",
    "#labels_train_y = label_encoder.fit_transform(train_data.sentiment)\n",
    "#labels_dev_y = label_encoder.fit_transform(dev_data.sentiment)\n",
    "# Labels integer-encoded into: positive = 2, neutral = 1, negative = 0\n",
    "#one_hot = OneHotEncoder(sparse=False)\n",
    "#train_y = one_hot.fit_transform(labels_train_y.reshape(len(labels_train_y), 1))\n",
    "#dev_y = one_hot.fit_transform(labels_dev_y.reshape(len(labels_dev_y), 1))\n",
    "\n",
    "# One Hot Encoded: Positive = [0,0,1], Neutral = [0,1,0], Negative = [1,0,0]\n",
    "lb = LabelBinarizer(sparse_output=False)\n",
    "train_y = lb.fit_transform(train_data.sentiment)\n",
    "dev_y = lb.fit_transform(dev_data.sentiment)\n",
    "#train_y = lb_train_y.reshape(len(lb_train_y), 1)\n",
    "#dev_y = lb_dev_y.reshape(len(lb_dev_y), 1)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label = argmax(dev_y, axis = 1)\n",
    "#multilabel_confusion_matrix(label, label)\n",
    "#train_y.argmax(axis=1)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_dict = {}\n",
    "with open(\"data/glove.6B.100d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26874 ,  0.17994 , -0.29083 , -0.72304 , -0.05883 ,  0.37211 ,\n",
       "        0.39979 ,  0.47827 , -0.41014 , -0.089043,  0.68457 ,  0.29088 ,\n",
       "        0.9661  ,  0.43289 ,  0.44254 , -1.1529  ,  0.15147 , -0.02307 ,\n",
       "       -1.2467  , -0.037292,  0.94212 ,  0.37771 ,  1.2369  ,  0.12327 ,\n",
       "       -0.33831 , -0.98651 ,  0.44322 ,  0.083459, -0.11953 , -0.057447,\n",
       "        0.6761  , -0.59646 , -0.3251  ,  0.53957 ,  0.66822 ,  0.082015,\n",
       "        0.42181 ,  0.62666 ,  0.038678,  0.089652, -0.53395 , -0.40426 ,\n",
       "       -0.060807,  0.14335 ,  0.53841 , -0.12983 ,  0.43699 , -0.077531,\n",
       "        0.20441 , -0.9894  , -0.080389, -0.13893 ,  0.046432,  1.6775  ,\n",
       "       -0.34565 , -1.7503  , -0.25442 , -0.28207 ,  1.2024  ,  1.0927  ,\n",
       "       -0.55076 ,  1.3852  , -0.74759 ,  0.96273 ,  0.69044 , -0.41462 ,\n",
       "        0.55676 ,  0.39588 ,  0.053647, -0.35503 , -0.3909  , -0.48323 ,\n",
       "       -0.048448, -0.37728 , -0.51204 ,  0.50097 ,  0.16188 ,  0.91052 ,\n",
       "       -1.6308  , -0.31484 ,  0.51824 , -0.078027, -0.33929 ,  0.42289 ,\n",
       "       -2.3287  , -0.56737 ,  0.17769 , -0.34047 , -0.75328 , -0.37805 ,\n",
       "       -0.45665 , -0.60386 , -0.41089 ,  0.078006, -1.3394  ,  0.049803,\n",
       "       -0.91783 , -0.47655 ,  0.79018 , -0.28336 ], dtype=float32)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['were']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word count in training data:  33\n",
      "Max word count in development data:  30\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/glove.6B.100d.txt')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(train_data['tweet_text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# find maximum length of each document\n",
    "train_lens = [len(x.split()) for x in train_data['tweet_text']]\n",
    "print(\"Max word count in training data: \", max(train_lens))\n",
    "dev_lens = [len(x.split()) for x in dev_data['tweet_text']]\n",
    "print(\"Max word count in development data: \", max(dev_lens))\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_data.tweet_text), maxlen=33) #avg word count of a tweet is 55 words\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(dev_data.tweet_text), maxlen=33)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = [i for i in range(len(lengths)) if lengths[i] > 32]\n",
    "#print(train_data['tweet_text'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings_dict is the same as embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=1)\n",
    "        \n",
    "    dev_y2 = dev_y.argmax(axis=1)\n",
    "        \n",
    "    print(confusion_matrix(dev_y2, predictions, labels = [2, 1, 0]))\n",
    "    \n",
    "    return accuracy_score(predictions, dev_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45101/45101 [==============================] - 290s 6ms/sample - loss: 0.9082 - acc: 0.5545 - categorical_accuracy: 0.5545\n",
      "[[310 381  12]\n",
      " [104 782  33]\n",
      " [ 18 261  99]]\n",
      "RNN-LSTM, Word Embeddings 0.5955\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_lstm():\n",
    "    # Add an Input Layer\n",
    "    input_layer = keras.layers.Input((33, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = keras.layers.Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = keras.layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = keras.layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = keras.layers.Dense(33, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = keras.layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = keras.layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy','categorical_accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
